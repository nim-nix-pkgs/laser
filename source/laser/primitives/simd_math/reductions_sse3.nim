# Laser
# Copyright (c) 2018 Mamy Andr√©-Ratsimbazafy
# Distributed under the Apache v2 License (license terms are at http://www.apache.org/licenses/LICENSE-2.0).
# This file may not be copied, modified, or distributed except according to those terms.

import
  ../../simd, ../../compiler_optim_hints,
  ../../private/align_unroller,
  ../private/sse3_utils

template reduction_op(op_name, initial_val, scalar_op, vector_op, merge_op: untyped) =
  func op_name*(data: ptr (float32 or UncheckedArray[float32]), len: Natural): float32 =
    ## Sum a contiguous range of float32 using SSE3 instructions
    withCompilerOptimHints()
    let data{.restrict.} = cast[ptr UncheckedArray[float32]](data)
    var vec_result = initial_val

    # Loop peeling, while not aligned to 16-byte boundary advance
    var idx = 0
    while (cast[ByteAddress](data[idx].addr) and 15) != 0:
      let data0 = data[idx].addr.mm_load_ss()
      vec_result = scalar_op(vec_result, data0)
      inc idx

    let data_aligned{.restrict.} = assume_aligned cast[ptr UncheckedArray[float32]](data[idx].addr)

    # Main vectorized and unrolled loop.
    const step = 16
    let new_end = len - idx
    let unroll_stop = round_step_down(new_end, step)
    var
      accum4_0 = initial_val
      accum4_1 = initial_val
      accum4_2 = initial_val
      accum4_3 = initial_val

    for i in countup(0, unroll_stop - 1, step):
      let
        data4_0 = data_aligned[i   ].addr.mm_load_ps()
        data4_1 = data_aligned[i+4 ].addr.mm_load_ps()
        data4_2 = data_aligned[i+8 ].addr.mm_load_ps()
        data4_3 = data_aligned[i+12].addr.mm_load_ps()
      accum4_0 = vector_op(accum4_0, data4_0)
      accum4_1 = vector_op(accum4_1, data4_1)
      accum4_2 = vector_op(accum4_2, data4_2)
      accum4_3 = vector_op(accum4_3, data4_3)
    accum4_0 = vector_op(accum4_0, accum4_1)
    accum4_2 = vector_op(accum4_2, accum4_3)
    accum4_0 = vector_op(accum4_0, accum4_2)

    for i in unroll_stop ..< new_end:
      let data0 = data_aligned[i].addr.mm_load_ss()
      vec_result = scalar_op(vec_result, data0)
    vec_result = scalar_op(vec_result, accum4_0.merge_op())
    result = vec_result.mm_cvtss_f32()

reduction_op(sum_sse3, mm_setzero_ps(), mm_add_ss, mm_add_ps, sum_ps_sse3)
reduction_op(max_sse3, mm_set1_ps(float32(-Inf)), mm_max_ss, mm_max_ps, max_ps_sse3)
reduction_op(min_sse3, mm_set1_ps(float32(Inf)), mm_min_ss, mm_min_ps, min_ps_sse3)

## Loop generated by Clang for sum - memory bandwith bottleneck after 64 Bytes are loaded?
# +0x4c	nopl                (%rax)
# +0x50	    vaddps              (%rdi,%rcx,4), %xmm0, %xmm0
# +0x55	    vaddps              16(%rdi,%rcx,4), %xmm1, %xmm1
# +0x5b	    vaddps              32(%rdi,%rcx,4), %xmm2, %xmm2
# +0x61	    vaddps              48(%rdi,%rcx,4), %xmm3, %xmm3
# +0x67	    vaddps              64(%rdi,%rcx,4), %xmm0, %xmm0
# +0x6d	    vaddps              80(%rdi,%rcx,4), %xmm1, %xmm1 # Bottleneck (or previous instruction)
# +0x73	    vaddps              96(%rdi,%rcx,4), %xmm2, %xmm2
# +0x79	    vaddps              112(%rdi,%rcx,4), %xmm3, %xmm3
# +0x7f	    addq                $32, %rcx
# +0x83	    addq                $2, %rdx
# +0x87	    jne                 "sum_sse3_StpaQVXnVtKoySxeCeYHRw+0x50"

